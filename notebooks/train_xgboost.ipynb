{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmach22/Promoter-Classification/blob/main/notebooks/train_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-header"
      },
      "source": [
        "# **Set Env**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-code"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    token = userdata.get('GITHUB_TOKEN')\n",
        "    user_name = userdata.get('GITHUB_USERNAME')\n",
        "    mail = userdata.get('GITHUB_MAIL')\n",
        "    !git config --global user.name \"{user_name}\"\n",
        "    !git config --global user.email \"{mail}\"\n",
        "    !git clone https://{token}@github.com/nmach22/Promoter-Classification.git\n",
        "except:\n",
        "    print(\"Github credentials not found, assuming repo is already cloned or local environment.\")\n",
        "\n",
        "!pip install -r ./Promoter-Classification/requirements.txt\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports-header"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports-code"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, classification_report\n",
        "from Bio import SeqIO\n",
        "import importlib\n",
        "\n",
        "# Add the root directory of the cloned repository to the Python path\n",
        "ROOT_DIR = '/content/Promoter-Classification' # Adjust if running locally\n",
        "if not os.path.exists(ROOT_DIR):\n",
        "    ROOT_DIR = os.path.abspath(\"../\") # Fallback for local run\n",
        "sys.path.append(ROOT_DIR)\n",
        "\n",
        "import utils.fasta_dataset as dataset_module\n",
        "import utils.encoding_functions as encoding_module\n",
        "import utils.data_split as splitter_module\n",
        "\n",
        "importlib.reload(encoding_module)\n",
        "\n",
        "from utils.fasta_dataset import FastaDataset\n",
        "from utils.encoding_functions import one_hot_encode, flatten_one_hot_encode, kmer_encode\n",
        "from utils.data_split import dataset_split\n",
        "\n",
        "with open(f\"{ROOT_DIR}/config/config.yaml\", \"r\") as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Config loaded.\")\n",
        "print(\"Device: CPU (XGBoost)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature-eng-header"
      },
      "source": [
        "# **Feature Engineering**\n",
        "\n",
        "XGBoost requires numerical input. We will use two types of features:\n",
        "1. **Flattened One-Hot Encoding**: Preserves positional information (critical for promoter motifs like TATA box at specific locations).\n",
        "2. **K-mer Counts**: Captures frequency of short subsequences (e.g., 3-mers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-eng-code"
      },
      "outputs": [],
      "source": [
        "# We use functions from utils.encoding_functions\n",
        "# flatten_one_hot_encode(seq, max_seq_len)\n",
        "# kmer_encode(seq, seq_len, k=3)\n",
        "\n",
        "def combined_features(seq, max_seq_len, k=3):\n",
        "    \"\"\"\n",
        "    Combines flattened one-hot and k-mer counts.\n",
        "    \"\"\"\n",
        "    one_hot = flatten_one_hot_encode(seq, max_seq_len)\n",
        "    # kmers = kmer_encode(seq, max_seq_len, k=k) # Uncomment to add k-mers\n",
        "    # return np.concatenate([one_hot, kmers])\n",
        "    return one_hot # Start with one-hot as primary feature\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-load-header"
      },
      "source": [
        "# **Read & Prepare Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-load-code"
      },
      "outputs": [],
      "source": [
        "# Select Dataset (e.g., E. coli)\n",
        "data_config = config['data']['ecoli']\n",
        "prom_path = f\"{ROOT_DIR}/{data_config['promoter_fasta']}\"\n",
        "non_prom_path = f\"{ROOT_DIR}/{data_config['non_promoter_fasta']}\"\n",
        "seq_length = data_config['seq_len']\n",
        "\n",
        "# Use FastaDataset\n",
        "dataset = FastaDataset(\n",
        "    prom_path, \n",
        "    non_prom_path, \n",
        "    seq_len=seq_length, \n",
        "    encoding_func=lambda s, l: combined_features(s, l)\n",
        ")\n",
        "\n",
        "print(f\"Total samples: {len(dataset)}\")\n",
        "print(f\"Feature vector shape: {dataset[0][0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "split-data-header"
      },
      "source": [
        "# **Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "split-data-code"
      },
      "outputs": [],
      "source": [
        "train_subset, val_subset, test_subset = dataset_split(dataset)\n",
        "\n",
        "def subset_to_numpy(subset):\n",
        "    \"\"\"\n",
        "    Converts a PyTorch Subset to numpy arrays (X, y).\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(subset)):\n",
        "        sample_x, sample_y = subset[i]\n",
        "        X.append(sample_x.numpy())\n",
        "        y.append(sample_y.item())\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(\"Converting datasets to numpy arrays for XGBoost...\")\n",
        "X_train, y_train = subset_to_numpy(train_subset)\n",
        "X_val, y_val = subset_to_numpy(val_subset)\n",
        "X_test, y_test = subset_to_numpy(test_subset)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Val shape: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test shape: {X_test.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "train-header"
      },
      "source": [
        "# **Train XGBoost Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-code"
      },
      "outputs": [],
      "source": [
        "# Initialize XGBClassifier\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric='logloss',\n",
        "    early_stopping_rounds=20,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Training...\")\n",
        "model.fit(\n",
        "    X_train, y_train, \n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)], \n",
        "    verbose=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval-header"
      },
      "source": [
        "# **Evaluation**\n",
        "\n",
        "Metrics from paper:\n",
        "- **Sn (Sensitivity)**: Recall\n",
        "- **Sp (Specificity)**: True Negative Rate\n",
        "- **CC**: Matthews Correlation Coefficient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval-code"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    \n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    \n",
        "    return {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Sensitivity (Sn)\": sensitivity,\n",
        "        \"Specificity (Sp)\": specificity,\n",
        "        \"CC (MCC)\": mcc,\n",
        "        \"Confusion Matrix\": cm\n",
        "    }\n",
        "\n",
        "# Predict on Test Set\n",
        "y_pred = model.predict(X_test)\n",
        "metrics = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "print(\"Test Results:\")\n",
        "print(\"-------------\")\n",
        "for k, v in metrics.items():\n",
        "    if k != \"Confusion Matrix\":\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{k}:\\n{v}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Non-Promoter', 'Promoter']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save-header"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-code"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "save_dir = f\"{ROOT_DIR}/models/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "save_path = f\"{save_dir}/xgboost_model.json\"\n",
        "model.save_model(save_path)\n",
        "print(f\"Model saved to {save_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
